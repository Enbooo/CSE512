{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "hw2_alice_naivebayes_release.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fk5Zw48_fJ7Y",
        "outputId": "340c862c-84b9-400e-dfce-e44db83630d2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive') "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_61oNyrfQ-w"
      },
      "source": [
        "import os\n",
        "code_path = \"/content/gdrive/MyDrive/cse512hw2Challenge/\"\n",
        "os.chdir(code_path)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtpdHeZjh1pZ"
      },
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "import random\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5-8LQOzh1pd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "909d67fe-f426-48bc-d459-757cedb303bb"
      },
      "source": [
        "corpus = []\n",
        "f = open('alice_in_wonderland.txt','r')\n",
        "while(1):\n",
        "    line =  f.readline()\n",
        "    if len(line) == 0: break\n",
        "    corpus.extend(line.split())\n",
        "        \n",
        "f.close()\n",
        "corpus = ' '.join(corpus)\n",
        "\n",
        "def clean_word(word):\n",
        "    word = word.lower()\n",
        "    for punctuation in ['\"',\"'\",'.',',','-','?','!',';',':','â€”','(',')','[',']']:\n",
        "        word = word.split(punctuation)[0]\n",
        "    return word\n",
        "\n",
        "\n",
        "\n",
        "corpus = [clean_word(word) for word in corpus.split()]\n",
        "corpus = [word for word in corpus if len(word) > 0]\n",
        "print(corpus[:25])\n",
        "D = len(corpus)\n",
        "print('corpus len: ',D)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['alice', 'adventures', 'in', 'wonderland', 'by', 'lewis', 'carroll', 'the', 'millennium', 'fulcrum', 'edition', '3', 'contents', 'chapter', 'i', 'down', 'the', 'rabbit', 'chapter', 'ii', 'the', 'pool', 'of', 'tears', 'chapter']\n",
            "corpus len:  25320\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mIUA54nh1pf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d873e7da-2829-4f2f-eeeb-e0fb32a146c1"
      },
      "source": [
        "tokenize = {}\n",
        "dictionary = []\n",
        "token = 0\n",
        "for word in corpus:\n",
        "    if word not in tokenize.keys():\n",
        "        tokenize[word] = token\n",
        "        dictionary.append(word)\n",
        "        token += 1\n",
        "    \n",
        "V = len(dictionary)\n",
        "print('dictionary size (number of distinct words): ', V)\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dictionary size (number of distinct words):  2501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARyNaN98h1pg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a638d04-d55e-47f1-99fa-df6f492f27e7"
      },
      "source": [
        "#past word as feature\n",
        "\n",
        "posterior_1word = np.zeros((V, V))\n",
        "prior = np.zeros(V)\n",
        "# Calculate the prior of the words.\n",
        "for i in range(len(corpus)):\n",
        "    prior[tokenize[corpus[i]]] += 1\n",
        "    if i > 0:\n",
        "       posterior_1word[tokenize[corpus[i-1]]][tokenize[corpus[i]]] += 1\n",
        "\n",
        "posterior_1word = posterior_1word / prior\n",
        "prior = prior / len(corpus)\n",
        "\n",
        "def get_likelihood_2gram(word):\n",
        "    likelihood = posterior_1word[tokenize[word], :] * prior\n",
        "    return(likelihood)\n",
        "def pred_2gram(word):\n",
        "    likelihood = get_likelihood_2gram(word)\n",
        "    i = np.argmax(likelihood)\n",
        "    return(dictionary[i], likelihood[i])\n",
        "print(pred_2gram('alice'))\n",
        "print(pred_2gram('the'))\n",
        "print(pred_2gram('cheshire'))\n",
        "print(pred_2gram('mock'))\n",
        "    "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('was', 0.0007109004739336493)\n",
            "('queen', 0.0027646129541864135)\n",
            "('cat', 0.00019747235387045816)\n",
            "('turtle', 0.0022511848341232226)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E4UipUfh1ph",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1435f0fe-66a2-4b17-84cb-38f98b5c2303"
      },
      "source": [
        "# Using the likelihoods computed from the bigram classiffer, and starting with a seed word \"alice\", \n",
        "# generate the next 25 words by always picking the most likely next word.\n",
        "word = \"alice\"\n",
        "article = word\n",
        "for i in range(25):\n",
        "    word, _ = pred_2gram(word)\n",
        "    article = article + \" \" + word\n",
        "print(article)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alice was a little thing i can remember ever saw in a little thing i can remember ever saw in a little thing i can remember\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvHOJOuAh1pi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5000c15d-8746-418b-b254-0677a6e4029a"
      },
      "source": [
        "# Using random choices method\n",
        "word = \"alice\"\n",
        "article = word\n",
        "for i in range(25):\n",
        "    likelihood = get_likelihood_2gram(word)\n",
        "    word = random.choices(dictionary, weights = likelihood)[0]\n",
        "    article = article + \" \" + word\n",
        "print(article)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alice had to have anything but on their faces at first witness said a sudden violence that it would not stand down at the silence alice\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCZznhTxfipB",
        "outputId": "c0c960e7-6c95-4782-883e-816450787d96"
      },
      "source": [
        "# Calculate the acc of 1 word.\n",
        "positive = 0\n",
        "for i in range(len(corpus) - 1):\n",
        "    if pred_2gram(corpus[i])[0] == corpus[i+1]:\n",
        "        positive += 1\n",
        "print(\"The accuracy:\", positive / (len(corpus)-1))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy: 0.2453493423910897\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHz0xgu5fkJE",
        "outputId": "4c65c6e4-77a6-4c64-af79-da93c98fded1"
      },
      "source": [
        "#past 2 words as features\n",
        "\n",
        "posterior_2words = np.zeros((V, V))\n",
        "for i in range(0, len(corpus) - 2):\n",
        "    posterior_2words[tokenize[corpus[i]]][tokenize[corpus[i + 2]]] += 1 \n",
        "posterior_2words /= prior\n",
        "\n",
        "posterior_2gram = np.vstack([posterior_1word,posterior_2words])\n",
        "\n",
        "\n",
        "\n",
        "def get_likelihood_3gram(word2ago,word1ago):\n",
        "    likelihood = posterior_1word[tokenize[word1ago], :] * posterior_2words[tokenize[word2ago], :] * prior\n",
        "    return likelihood\n",
        "def pred_3gram(word2ago,word1ago):\n",
        "    likelihood = get_likelihood_3gram(word2ago,word1ago)\n",
        "    i = np.argmax(likelihood)\n",
        "    return dictionary[i], likelihood[i]\n",
        "print(pred_3gram('pack','of'))\n",
        "print(pred_3gram('the','mad'))\n",
        "print(pred_3gram('she','jumped'))\n",
        "\n",
        "    "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('cards', 3.0)\n",
            "('you', 0.14447592067988668)\n",
            "('up', 0.5416666666666666)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXhdIFPuflrU",
        "outputId": "a8f70767-6fc7-4761-957f-ab2c7ead9cd3"
      },
      "source": [
        "first_word = \"alice\"\n",
        "second_word = \"was\"\n",
        "article = first_word + \" \" + second_word\n",
        "for i in range(25):\n",
        "    new, _ = pred_3gram(first_word, second_word)\n",
        "    article = article + \" \" + new\n",
        "    first_word = second_word\n",
        "    second_word = new\n",
        "print(article)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alice was not easy to take this young lady tells us a story afraid i am i ah that the queen who was peeping anxiously into its\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUS1B2okfnFb",
        "outputId": "bac95b40-1e8e-48dc-e51f-dbd2716d08a0"
      },
      "source": [
        "# Using random choices method\n",
        "first_word = \"alice\"\n",
        "second_word = \"was\"\n",
        "article = first_word + \" \" + second_word\n",
        "for i in range(25):\n",
        "    likelihood = get_likelihood_3gram(first_word, second_word)\n",
        "    new = random.choices(dictionary, weights = likelihood)[0]\n",
        "    article = article + \" \" + new\n",
        "    first_word = second_word\n",
        "    second_word = new\n",
        "print(article)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alice was thoroughly puzzled the very interesting dance to you myself you begin for your interesting story but she was not otherwise than what am i ah\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ml8FTUs7fohP",
        "outputId": "1a63cd65-019d-4e6a-bff5-f538b5f6f94a"
      },
      "source": [
        "# Calculate the acc of 2 word.\n",
        "positive = 0\n",
        "for i in range(len(corpus) - 2):\n",
        "    if pred_3gram(corpus[i], corpus[i + 1])[0] == corpus[i+2]:\n",
        "        positive += 1\n",
        "print(\"The accuracy:\", positive / (len(corpus)-2))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy: 0.5047397108776365\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC9gPyT2ItXd"
      },
      "source": [
        "# Challenge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikTXQi1aIvsR"
      },
      "source": [
        "posterior_n_words = []\n",
        "for i in range(1, 101):\n",
        "    posterior_i_words = np.zeros((V, V))\n",
        "    for j in range(0, len(corpus) - i):\n",
        "        posterior_i_words[tokenize[corpus[j]]][tokenize[corpus[j + i]]] += 1 \n",
        "    posterior_i_words /= prior\n",
        "    posterior_n_words.append(posterior_i_words)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kso6leoLIx3L"
      },
      "source": [
        "def get_likelihood_n_gram(word_n_agos):\n",
        "    n = len(word_n_agos)\n",
        "    likelihood = 1\n",
        "    for i in range(n):\n",
        "        likelihood *= posterior_n_words[i][tokenize[word_n_agos[i]], :]\n",
        "    likelihood *= prior\n",
        "    return likelihood\n",
        "def pred_n_gram(word_n_agos):\n",
        "    likelihood = get_likelihood_n_gram(word_n_agos)\n",
        "    i = np.argmax(likelihood)\n",
        "    return dictionary[i], likelihood[i]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bTsWXltIydA",
        "outputId": "238b0d0f-f0af-43d7-8c40-addadc972fa0"
      },
      "source": [
        "# Calculate the acc of n word.\n",
        "n_gram = 100\n",
        "positive = 0\n",
        "for i in range(len(corpus) - n_gram):\n",
        "    if pred_n_gram(corpus[i:i+n_gram][::-1])[0] == corpus[i+n_gram]:\n",
        "        positive += 1\n",
        "print(\"The accuracy:\", positive / (len(corpus) - n_gram))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: overflow encountered in multiply\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDl7pKJCI1cR",
        "outputId": "01808c96-4d5a-4cbc-9da9-f859a505cff3"
      },
      "source": [
        "for n_gram in range(1, 101):\n",
        "    positive = 0\n",
        "    for i in range(len(corpus) - n_gram):\n",
        "        if pred_n_gram(corpus[i:i+n_gram][::-1])[0] == corpus[i+n_gram]:\n",
        "            positive += 1\n",
        "    print(\"n_gram = \", n_gram, \" The accuracy:\", positive / (len(corpus) - n_gram))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_gram =  1  The accuracy: 0.2453493423910897\n",
            "n_gram =  2  The accuracy: 0.5047002132869894\n",
            "n_gram =  3  The accuracy: 0.7499703756369238\n",
            "n_gram =  4  The accuracy: 0.8784563122136199\n",
            "n_gram =  5  The accuracy: 0.9401935611297649\n",
            "n_gram =  6  The accuracy: 0.9667772773959074\n",
            "n_gram =  7  The accuracy: 0.9831311974084462\n",
            "n_gram =  8  The accuracy: 0.9897281921618205\n",
            "n_gram =  9  The accuracy: 0.9937971632886887\n",
            "n_gram =  10  The accuracy: 0.9960489924930858\n",
            "n_gram =  11  The accuracy: 0.997313208739974\n",
            "n_gram =  12  The accuracy: 0.9983404457088667\n",
            "n_gram =  13  The accuracy: 0.9988935867546529\n",
            "n_gram =  14  The accuracy: 0.9991701572749545\n",
            "n_gram =  15  The accuracy: 0.999407231772377\n",
            "n_gram =  16  The accuracy: 0.9995652861207714\n",
            "n_gram =  17  The accuracy: 0.9996443109512706\n",
            "n_gram =  18  The accuracy: 0.9996442968935262\n",
            "n_gram =  19  The accuracy: 0.9998023793525948\n",
            "n_gram =  20  The accuracy: 0.9998418972332016\n",
            "n_gram =  21  The accuracy: 0.9999209454919167\n",
            "n_gram =  22  The accuracy: 0.9999604711834927\n",
            "n_gram =  23  The accuracy: 0.9999604696209037\n",
            "n_gram =  24  The accuracy: 0.999960468058191\n",
            "n_gram =  25  The accuracy: 0.9999604664953549\n",
            "n_gram =  26  The accuracy: 0.999960464932395\n",
            "n_gram =  27  The accuracy: 0.9999604633693117\n",
            "n_gram =  28  The accuracy: 0.9999604618061047\n",
            "n_gram =  29  The accuracy: 0.9999604602427741\n",
            "n_gram =  30  The accuracy: 0.9999604586793199\n",
            "n_gram =  31  The accuracy: 0.999960457115742\n",
            "n_gram =  32  The accuracy: 1.0\n",
            "n_gram =  33  The accuracy: 1.0\n",
            "n_gram =  34  The accuracy: 1.0\n",
            "n_gram =  35  The accuracy: 1.0\n",
            "n_gram =  36  The accuracy: 1.0\n",
            "n_gram =  37  The accuracy: 1.0\n",
            "n_gram =  38  The accuracy: 1.0\n",
            "n_gram =  39  The accuracy: 1.0\n",
            "n_gram =  40  The accuracy: 1.0\n",
            "n_gram =  41  The accuracy: 1.0\n",
            "n_gram =  42  The accuracy: 1.0\n",
            "n_gram =  43  The accuracy: 1.0\n",
            "n_gram =  44  The accuracy: 1.0\n",
            "n_gram =  45  The accuracy: 1.0\n",
            "n_gram =  46  The accuracy: 1.0\n",
            "n_gram =  47  The accuracy: 1.0\n",
            "n_gram =  48  The accuracy: 1.0\n",
            "n_gram =  49  The accuracy: 1.0\n",
            "n_gram =  50  The accuracy: 1.0\n",
            "n_gram =  51  The accuracy: 1.0\n",
            "n_gram =  52  The accuracy: 1.0\n",
            "n_gram =  53  The accuracy: 1.0\n",
            "n_gram =  54  The accuracy: 1.0\n",
            "n_gram =  55  The accuracy: 1.0\n",
            "n_gram =  56  The accuracy: 1.0\n",
            "n_gram =  57  The accuracy: 1.0\n",
            "n_gram =  58  The accuracy: 1.0\n",
            "n_gram =  59  The accuracy: 1.0\n",
            "n_gram =  60  The accuracy: 1.0\n",
            "n_gram =  61  The accuracy: 1.0\n",
            "n_gram =  62  The accuracy: 1.0\n",
            "n_gram =  63  The accuracy: 1.0\n",
            "n_gram =  64  The accuracy: 1.0\n",
            "n_gram =  65  The accuracy: 1.0\n",
            "n_gram =  66  The accuracy: 1.0\n",
            "n_gram =  67  The accuracy: 1.0\n",
            "n_gram =  68  The accuracy: 1.0\n",
            "n_gram =  69  The accuracy: 1.0\n",
            "n_gram =  70  The accuracy: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: overflow encountered in multiply\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_gram =  71  The accuracy: 1.0\n",
            "n_gram =  72  The accuracy: 1.0\n",
            "n_gram =  73  The accuracy: 1.0\n",
            "n_gram =  74  The accuracy: 1.0\n",
            "n_gram =  75  The accuracy: 1.0\n",
            "n_gram =  76  The accuracy: 1.0\n",
            "n_gram =  77  The accuracy: 1.0\n",
            "n_gram =  78  The accuracy: 1.0\n",
            "n_gram =  79  The accuracy: 1.0\n",
            "n_gram =  80  The accuracy: 1.0\n",
            "n_gram =  81  The accuracy: 1.0\n",
            "n_gram =  82  The accuracy: 1.0\n",
            "n_gram =  83  The accuracy: 1.0\n",
            "n_gram =  84  The accuracy: 1.0\n",
            "n_gram =  85  The accuracy: 1.0\n",
            "n_gram =  86  The accuracy: 1.0\n",
            "n_gram =  87  The accuracy: 1.0\n",
            "n_gram =  88  The accuracy: 1.0\n",
            "n_gram =  89  The accuracy: 1.0\n",
            "n_gram =  90  The accuracy: 1.0\n",
            "n_gram =  91  The accuracy: 1.0\n",
            "n_gram =  92  The accuracy: 1.0\n",
            "n_gram =  93  The accuracy: 1.0\n",
            "n_gram =  94  The accuracy: 1.0\n",
            "n_gram =  95  The accuracy: 1.0\n",
            "n_gram =  96  The accuracy: 1.0\n",
            "n_gram =  97  The accuracy: 1.0\n",
            "n_gram =  98  The accuracy: 1.0\n",
            "n_gram =  99  The accuracy: 1.0\n",
            "n_gram =  100  The accuracy: 1.0\n"
          ]
        }
      ]
    }
  ]
}